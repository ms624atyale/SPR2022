{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMH48u46nV3gCRAGqLyVXyG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms624atyale/SPR2022/blob/main/Corpus_Toolkit_Annotation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Corpus (코퍼스/말뭉치) \n",
        "- 코퍼스는 언어의 본질을 보여줄 수 있도록 문장 자료를 모아놓은 것이다. 코퍼스로 분석할 수 있는 것들은 i) 단어수 계산, ii) n-gram (코퍼스에서 n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 취급) 빈도수와 범워, iii) 키값, iv) collocation (연어), iv) 의존합자식별(예: 동사-직접목적어 조합) 및 의존합자식별 빈도-범위-연결강도 등이 있다.  \n",
        "\n",
        "- 위의 분석을 위해서는 corpus-toolkit 패키지를 설치하거나 colab에 이미 기본적으로 설치되어 있는 패키지에서 분석에 필요한 내부 함수를 적절하게 코드 작성 때 사용한다.   \n",
        "-- corpus-toolkit 패키지의 tokenization(토큰화) & lemmatization(표준형태화) 사용\n",
        "-- colab에 이미 설치되어 있는 tagging(단어에 문법범주 연결) & parsing(구분자를 사용하여 문자열을 구성 요소로 분석) 사용"
      ],
      "metadata": {
        "id": "M7afPLwh-bRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab 작업 디렉토리에서 폴더를 생성하여 다른 곳에서 다운로드 받은 단/복수의 파일을 옮겨 넣는 방법 \n",
        "- [새폴더 만들기] 코랩에 기본으로 깔려 있는 sample_data 에 커서를 대면 활성화되면서 우측에 세로 점 3 개가 나타난다. 거기를 클릭하면 선택사항 -> new folder를 클릭한다. \n",
        "- [폴더를 샘플 디렉토리와 같은 레벨로 이동] new folder에 커서를 대면 활성화되면서 우측에 세로 점 3 개가 나타난다. 해당 폴더에 커서를 대고 누르고 있으면 하단에 <Drag file and ...> 창이 나타나는데, 여기로 해당 폴더를 끌어다 넣는다.\n",
        "- [폴더 새이름]해당 폴더에 Rename folder 클릭해서 파일명을 새이름으로 바꾼다. 예)brown_single\n",
        "- [폴더에 파일 업로드] 해당 폴더에 커서를 대면 활성화되면서 우측에 세로 점 3 개가 나타난다. Upload 클릭해서 본인 컴퓨터에 다운로드 받은 복수의 파일을 (혹은 기존 파일) 모두 업로드 한다. "
      ],
      "metadata": {
        "id": "IpAwLbK32qeN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBz4z96Z-YV5",
        "outputId": "c5d12833-09d7-4431-d69e-4ea44e7c3e1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting corpus-toolkit\n",
            "  Downloading corpus_toolkit-0.32-py3-none-any.whl (1.7 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 33.5 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 37.8 MB/s eta 0:00:01\r\u001b[K     |▊                               | 40 kB 38.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 33.7 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 35.0 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81 kB 35.1 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 92 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 112 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 122 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 133 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 153 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 163 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 174 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 184 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 194 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 204 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 215 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 225 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 235 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 245 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 256 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████                           | 266 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 276 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 286 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 296 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 307 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 317 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 327 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 337 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 348 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 358 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 368 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 378 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 389 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 399 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 409 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 419 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 430 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 440 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 450 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 460 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 471 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 481 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 491 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 501 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 512 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 522 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 532 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 542 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 552 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 563 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 573 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 583 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 593 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 604 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 614 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 624 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 634 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 645 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 655 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 665 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 675 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 686 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 696 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 706 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 716 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 727 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 737 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 747 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 757 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 768 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 778 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 788 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 798 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 808 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 819 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 829 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 839 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 849 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 860 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 870 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 880 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 890 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 901 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 911 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 921 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 931 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 942 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 952 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 962 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 972 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 983 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 993 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.0 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.0 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.0 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.0 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.1 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.2 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.2 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.2 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.2 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.3 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.3 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.3 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.4 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.4 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.4 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.4 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.4 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.4 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.4 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.4 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.5 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.5 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.5 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.5 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.5 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.5 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.5 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.6 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.6 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.6 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.6 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.6 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.6 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.7 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.7 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.7 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.7 MB 34.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 34.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: corpus-toolkit\n",
            "Successfully installed corpus-toolkit-0.32\n"
          ]
        }
      ],
      "source": [
        "pip install corpus-toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-processing (사전처리)\n",
        "-- 코퍼스 분석을 하려면 분석하려는 텍스트의 단어를 토큰화하여 분석할 수 있는 형태로 준비해 놓아야 한다. i) tokenize 함수를 사용하여 토큰을 리스트화 하여 준비해 놓을 수도 있고, ii) tag 함수를 사용하여 토큰에 문법범주(품사)를 연결한 형태로 준비해 놓을 수도 있다.  \n",
        "####[코드문법1: from~ import ~ as ~] \n",
        "-- corpus_toolkit패키지에서 corpus_tools모듈을 ct로 줄여서 불러들여라.\n",
        "####[코드문법2: 파일 올리고 읽기] ldcorpus( )는 파일 읽기 \n",
        "-- ct모듈의 ldcorpus( )함수에 brwon_single 데이터 폴더를 인자로 넣어, 그 결과를 brown_corp변수에 할당한다.\n",
        "####[코드문법3: 단어 토큰화] tokenize( )는 데이터 토큰화\n",
        "-- ct모듈의 tokenize( )함수 인자로 brown_corp 변수를 넣어, 그 결과를 tok_corp변수에 할당한다.\n",
        "####[코드문법4: 토큰화된 단어 빈도수 계산] : frequency( )는 토큰의 빈도수 계산\n",
        "-- ct모듈의 frequency( )함수 인자로 tok_corp 변수를 넣어, 그 결과를 brown_freq변수에 할당한다.\n",
        "####[코드문법5: 빈도수 높은 10 개 출력]\n",
        "-- ct모듈의 head( )함수에 brown_freq변수를 첫번째 인자로, 빈도수가 높은 것 10 개를 두번째 인자로 사용해서 (hits = 10), 상위 10 개 토큰의 빈도수를 출력한다."
      ],
      "metadata": {
        "id": "bO38yLp90864"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from corpus_toolkit import corpus_tools as ct\n",
        "brown_corp = ct.ldcorpus(\"brown_single\") #load and read corpus\n",
        "tok_corp = ct.tokenize(brown_corp) #tokenize corpus - by default this lemmatizes as well\n",
        "brown_freq = ct.frequency(tok_corp) #creates a frequency dictionary\n",
        "##note that range can be calculated instead of frequency using the argument calc = \"range\"\n",
        "ct.head(brown_freq, hits = 10) #print top 10 items"
      ],
      "metadata": {
        "id": "17lsll3VIzu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nest (차곡히 쌓기): 여러 줄의 명령어에 대하여 괄호를 계층적으로 사용하여 한 줄 명령어로 작성    \n",
        "위 명령창에 코드를 작성 때에는 \"모듈.함수( )\"가 한 세트인 명령어 사용하여 i) 파일을 로드해서 읽고 -> ii) 토큰화 하고 -> iii) 토큰의 빈도수를 계산하는 명령어를 사용하여 코드로 작성했다. 일련의 코드는 Nest 기법을 사용해서 한 줄 짜리 코드로 작성이 가능하다. 예시는 아래에 있다.  \n"
      ],
      "metadata": {
        "id": "R-98Zbi6j8qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown_freq = ct.frequency(ct.tokenize(ct.ldcorpus(\"brown_single\")))\n",
        "ct.head(brown_freq, hits = 10)"
      ],
      "metadata": {
        "id": "zxrWoaKbj-Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Concord \n",
        "\n",
        "### 1. Concord (단어 용례 분석) \n",
        "텍스트에서 관심 있는 특정 단어가 어떤 맥락에서 사용되는 지 알기 위해서 함께 나오는 단어를  방법이 있다. 여기에 다른 단어를 추가하여 특정 단어가 나타난 환경을 더 세부적으로 분석할 수도 있다. \n",
        "\n",
        "### 2. Concord (단어 용법 분석)을 할 때에 세 가지를 지정해 준다. \n",
        "- 맥락에서 함께 등장하는 단어를 출력할 때 텍스트에 나온 형태 그대로 사용하고자 할 때에는 (lemma=False)라고 지정\n",
        "- 분석 대상이 되는 단어는 \" \" 안에 넣어 문자열의 형태로 리스트 [ ] 안에 쉼표를 사용해서 넣어 준다. (예시 [\"run\", \"ran\", 'running\", \"runs\"]\n",
        "- 특정 단어는 그 앞에 10 단어와 그 뒤에 10 단어가 기본값으로 출력되는데, 이 때 보고 싶은 라인별 출력량은 nhits에 원하는 숫자를 쓰면 된다. 무작위로 생성되며 1 ~ N 으로 입력 가능하다.  \n",
        "\n",
        "[코드분석 1] 파일 읽기 -> 토큰화 -> 단어 리스트에 포함된 [\"run\", \"ran\", \"running\", \"runs\"] 의 용법을 알아볼 수 있는 함께 나오는 단어 뽑되 그 결과를 무작위로 10 개 뽑는다. 이를 con_results 변수에 할당한다. \n",
        "- ldcorpus ( ) 함수로 brwon_single 파일을 읽기\n",
        "- tokenize ( ) 함수로 텍스트 단어를 토큰화\n",
        "- concord ( ) 함수로 단어 용례 환경을 출력  \n",
        "- 결과를 한 개 씩 보기 좋게 출력하기 위하여 for 함수를 사용한다. con_results 변수에서 찾는 단어 나타날 때 마다 그 결과를 각각 출력한다.    "
      ],
      "metadata": {
        "id": "c7caiVBZO5d7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results1 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"],nhits = 10)\n",
        "for x in conc_results1:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "id": "_z3M-DivngQ-",
        "outputId": "3f448c56-a929-4fc6-f7fc-da2d8f08d60d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing cf_cf18.txt (1 of 500 files)\n",
            "Processing cf_cf30.txt (2 of 500 files)\n",
            "Processing cd_cd03.txt (3 of 500 files)\n",
            "Processing ck_ck13.txt (4 of 500 files)\n",
            "Processing cf_cf06.txt (5 of 500 files)\n",
            "Processing cf_cf29.txt (6 of 500 files)\n",
            "Processing cp_cp09.txt (7 of 500 files)\n",
            "Processing cl_cl11.txt (8 of 500 files)\n",
            "Processing ce_ce32.txt (9 of 500 files)\n",
            "Processing ca_ca01.txt (10 of 500 files)\n",
            "Processing cf_cf43.txt (11 of 500 files)\n",
            "Processing cj_cj12.txt (12 of 500 files)\n",
            "Processing cj_cj61.txt (13 of 500 files)\n",
            "Processing cc_cc15.txt (14 of 500 files)\n",
            "Processing cp_cp25.txt (15 of 500 files)\n",
            "Processing ca_ca34.txt (16 of 500 files)\n",
            "Processing cp_cp27.txt (17 of 500 files)\n",
            "Processing ca_ca15.txt (18 of 500 files)\n",
            "Processing cb_cb03.txt (19 of 500 files)\n",
            "Processing cd_cd08.txt (20 of 500 files)\n",
            "Processing cn_cn25.txt (21 of 500 files)\n",
            "Processing cc_cc16.txt (22 of 500 files)\n",
            "Processing cf_cf11.txt (23 of 500 files)\n",
            "Processing cg_cg01.txt (24 of 500 files)\n",
            "Processing ce_ce33.txt (25 of 500 files)\n",
            "Processing ck_ck28.txt (26 of 500 files)\n",
            "Processing cm_cm04.txt (27 of 500 files)\n",
            "Processing cb_cb09.txt (28 of 500 files)\n",
            "Processing cj_cj67.txt (29 of 500 files)\n",
            "Processing cd_cd16.txt (30 of 500 files)\n",
            "Processing cj_cj42.txt (31 of 500 files)\n",
            "Processing cj_cj41.txt (32 of 500 files)\n",
            "Processing ch_ch07.txt (33 of 500 files)\n",
            "Processing ca_ca39.txt (34 of 500 files)\n",
            "Processing cf_cf07.txt (35 of 500 files)\n",
            "Processing ca_ca03.txt (36 of 500 files)\n",
            "Processing ch_ch24.txt (37 of 500 files)\n",
            "Processing cb_cb13.txt (38 of 500 files)\n",
            "Processing ck_ck26.txt (39 of 500 files)\n",
            "Processing cj_cj55.txt (40 of 500 files)\n",
            "Processing cg_cg50.txt (41 of 500 files)\n",
            "Processing cd_cd12.txt (42 of 500 files)\n",
            "Processing ca_ca04.txt (43 of 500 files)\n",
            "Processing cj_cj46.txt (44 of 500 files)\n",
            "Processing ca_ca31.txt (45 of 500 files)\n",
            "Processing cg_cg62.txt (46 of 500 files)\n",
            "Processing ch_ch14.txt (47 of 500 files)\n",
            "Processing cg_cg05.txt (48 of 500 files)\n",
            "Processing cp_cp21.txt (49 of 500 files)\n",
            "Processing cr_cr08.txt (50 of 500 files)\n",
            "Processing ch_ch27.txt (51 of 500 files)\n",
            "Processing cl_cl07.txt (52 of 500 files)\n",
            "Processing cj_cj53.txt (53 of 500 files)\n",
            "Processing cf_cf09.txt (54 of 500 files)\n",
            "Processing cl_cl23.txt (55 of 500 files)\n",
            "Processing cj_cj05.txt (56 of 500 files)\n",
            "Processing cl_cl06.txt (57 of 500 files)\n",
            "Processing ch_ch03.txt (58 of 500 files)\n",
            "Processing cg_cg23.txt (59 of 500 files)\n",
            "Processing cg_cg18.txt (60 of 500 files)\n",
            "Processing cg_cg73.txt (61 of 500 files)\n",
            "Processing cd_cd11.txt (62 of 500 files)\n",
            "Processing cl_cl17.txt (63 of 500 files)\n",
            "Processing cg_cg02.txt (64 of 500 files)\n",
            "Processing ch_ch15.txt (65 of 500 files)\n",
            "Processing cp_cp29.txt (66 of 500 files)\n",
            "Processing cg_cg38.txt (67 of 500 files)\n",
            "Processing cn_cn11.txt (68 of 500 files)\n",
            "Processing ce_ce31.txt (69 of 500 files)\n",
            "Processing cl_cl18.txt (70 of 500 files)\n",
            "Processing ce_ce17.txt (71 of 500 files)\n",
            "Processing ca_ca10.txt (72 of 500 files)\n",
            "Processing cg_cg41.txt (73 of 500 files)\n",
            "Processing cb_cb02.txt (74 of 500 files)\n",
            "Processing ck_ck05.txt (75 of 500 files)\n",
            "Processing cr_cr04.txt (76 of 500 files)\n",
            "Processing cj_cj07.txt (77 of 500 files)\n",
            "Processing cf_cf40.txt (78 of 500 files)\n",
            "Processing cj_cj68.txt (79 of 500 files)\n",
            "Processing cf_cf28.txt (80 of 500 files)\n",
            "Processing cg_cg16.txt (81 of 500 files)\n",
            "Processing cj_cj14.txt (82 of 500 files)\n",
            "Processing cp_cp11.txt (83 of 500 files)\n",
            "Processing cn_cn29.txt (84 of 500 files)\n",
            "Processing ca_ca11.txt (85 of 500 files)\n",
            "Processing cj_cj03.txt (86 of 500 files)\n",
            "Processing cb_cb05.txt (87 of 500 files)\n",
            "Processing cb_cb16.txt (88 of 500 files)\n",
            "Processing cf_cf14.txt (89 of 500 files)\n",
            "Processing cn_cn22.txt (90 of 500 files)\n",
            "Processing ch_ch11.txt (91 of 500 files)\n",
            "Processing ca_ca36.txt (92 of 500 files)\n",
            "Processing cp_cp23.txt (93 of 500 files)\n",
            "Processing cp_cp16.txt (94 of 500 files)\n",
            "Processing cg_cg31.txt (95 of 500 files)\n",
            "Processing cj_cj24.txt (96 of 500 files)\n",
            "Processing ca_ca30.txt (97 of 500 files)\n",
            "Processing cg_cg74.txt (98 of 500 files)\n",
            "Processing cp_cp15.txt (99 of 500 files)\n",
            "Processing ca_ca14.txt (100 of 500 files)\n",
            "Processing cg_cg61.txt (101 of 500 files)\n",
            "Processing cg_cg59.txt (102 of 500 files)\n",
            "Processing cg_cg58.txt (103 of 500 files)\n",
            "Processing cg_cg52.txt (104 of 500 files)\n",
            "Processing cd_cd05.txt (105 of 500 files)\n",
            "Processing ca_ca26.txt (106 of 500 files)\n",
            "Processing cf_cf23.txt (107 of 500 files)\n",
            "Processing cj_cj34.txt (108 of 500 files)\n",
            "Processing cg_cg70.txt (109 of 500 files)\n",
            "Processing cf_cf42.txt (110 of 500 files)\n",
            "Processing cd_cd02.txt (111 of 500 files)\n",
            "Processing cn_cn21.txt (112 of 500 files)\n",
            "Processing cn_cn05.txt (113 of 500 files)\n",
            "Processing cf_cf39.txt (114 of 500 files)\n",
            "Processing ca_ca29.txt (115 of 500 files)\n",
            "Processing cg_cg39.txt (116 of 500 files)\n",
            "Processing ce_ce12.txt (117 of 500 files)\n",
            "Processing ch_ch18.txt (118 of 500 files)\n",
            "Processing cg_cg17.txt (119 of 500 files)\n",
            "Processing cn_cn09.txt (120 of 500 files)\n",
            "Processing cj_cj08.txt (121 of 500 files)\n",
            "Processing cr_cr05.txt (122 of 500 files)\n",
            "Processing cj_cj60.txt (123 of 500 files)\n",
            "Processing ca_ca24.txt (124 of 500 files)\n",
            "Processing ca_ca17.txt (125 of 500 files)\n",
            "Processing cf_cf46.txt (126 of 500 files)\n",
            "Processing ce_ce06.txt (127 of 500 files)\n",
            "Processing ck_ck25.txt (128 of 500 files)\n",
            "Processing cg_cg29.txt (129 of 500 files)\n",
            "Processing cj_cj54.txt (130 of 500 files)\n",
            "Processing cl_cl22.txt (131 of 500 files)\n",
            "Processing cd_cd04.txt (132 of 500 files)\n",
            "Processing cp_cp18.txt (133 of 500 files)\n",
            "Processing cg_cg63.txt (134 of 500 files)\n",
            "Processing cb_cb18.txt (135 of 500 files)\n",
            "Processing ck_ck21.txt (136 of 500 files)\n",
            "Processing cf_cf20.txt (137 of 500 files)\n",
            "Processing cl_cl08.txt (138 of 500 files)\n",
            "Processing cj_cj80.txt (139 of 500 files)\n",
            "Processing cf_cf34.txt (140 of 500 files)\n",
            "Processing ck_ck10.txt (141 of 500 files)\n",
            "Processing cj_cj06.txt (142 of 500 files)\n",
            "Processing ch_ch06.txt (143 of 500 files)\n",
            "Processing cg_cg55.txt (144 of 500 files)\n",
            "Processing cn_cn08.txt (145 of 500 files)\n",
            "Processing cc_cc13.txt (146 of 500 files)\n",
            "Processing ch_ch10.txt (147 of 500 files)\n",
            "Processing cj_cj62.txt (148 of 500 files)\n",
            "Processing cd_cd14.txt (149 of 500 files)\n",
            "Processing cj_cj32.txt (150 of 500 files)\n",
            "Processing cj_cj44.txt (151 of 500 files)\n",
            "Processing cb_cb25.txt (152 of 500 files)\n",
            "Processing ch_ch01.txt (153 of 500 files)\n",
            "Processing ca_ca33.txt (154 of 500 files)\n",
            "Processing ca_ca43.txt (155 of 500 files)\n",
            "Processing cg_cg24.txt (156 of 500 files)\n",
            "Processing cn_cn15.txt (157 of 500 files)\n",
            "Processing cc_cc05.txt (158 of 500 files)\n",
            "Processing cj_cj70.txt (159 of 500 files)\n",
            "Processing cp_cp04.txt (160 of 500 files)\n",
            "Processing cm_cm05.txt (161 of 500 files)\n",
            "Processing cp_cp26.txt (162 of 500 files)\n",
            "Processing ce_ce02.txt (163 of 500 files)\n",
            "Processing cg_cg10.txt (164 of 500 files)\n",
            "Processing cb_cb21.txt (165 of 500 files)\n",
            "Processing cd_cd09.txt (166 of 500 files)\n",
            "Processing cj_cj23.txt (167 of 500 files)\n",
            "Processing ck_ck04.txt (168 of 500 files)\n",
            "Processing cf_cf05.txt (169 of 500 files)\n",
            "Processing cj_cj77.txt (170 of 500 files)\n",
            "Processing ca_ca07.txt (171 of 500 files)\n",
            "Processing cn_cn13.txt (172 of 500 files)\n",
            "Processing cl_cl03.txt (173 of 500 files)\n",
            "Processing cj_cj28.txt (174 of 500 files)\n",
            "Processing cj_cj52.txt (175 of 500 files)\n",
            "Processing cf_cf31.txt (176 of 500 files)\n",
            "Processing ch_ch22.txt (177 of 500 files)\n",
            "Processing ce_ce03.txt (178 of 500 files)\n",
            "Processing cp_cp28.txt (179 of 500 files)\n",
            "Processing cc_cc03.txt (180 of 500 files)\n",
            "Processing ck_ck01.txt (181 of 500 files)\n",
            "Processing cj_cj73.txt (182 of 500 files)\n",
            "Processing cf_cf33.txt (183 of 500 files)\n",
            "Processing cl_cl20.txt (184 of 500 files)\n",
            "Processing ck_ck17.txt (185 of 500 files)\n",
            "Processing ch_ch26.txt (186 of 500 files)\n",
            "Processing cj_cj40.txt (187 of 500 files)\n",
            "Processing cc_cc07.txt (188 of 500 files)\n",
            "Processing cb_cb10.txt (189 of 500 files)\n",
            "Processing ca_ca35.txt (190 of 500 files)\n",
            "Processing ca_ca18.txt (191 of 500 files)\n",
            "Processing cj_cj10.txt (192 of 500 files)\n",
            "Processing cj_cj58.txt (193 of 500 files)\n",
            "Processing ck_ck09.txt (194 of 500 files)\n",
            "Processing cg_cg09.txt (195 of 500 files)\n",
            "Processing ca_ca38.txt (196 of 500 files)\n",
            "Processing ce_ce24.txt (197 of 500 files)\n",
            "Processing cp_cp01.txt (198 of 500 files)\n",
            "Processing cf_cf19.txt (199 of 500 files)\n",
            "Processing cj_cj27.txt (200 of 500 files)\n",
            "Processing ch_ch25.txt (201 of 500 files)\n",
            "Processing ce_ce10.txt (202 of 500 files)\n",
            "Processing cn_cn17.txt (203 of 500 files)\n",
            "Processing cc_cc01.txt (204 of 500 files)\n",
            "Processing ca_ca41.txt (205 of 500 files)\n",
            "Processing cd_cd15.txt (206 of 500 files)\n",
            "Processing cj_cj45.txt (207 of 500 files)\n",
            "Processing ce_ce08.txt (208 of 500 files)\n",
            "Processing cf_cf13.txt (209 of 500 files)\n",
            "Processing ca_ca09.txt (210 of 500 files)\n",
            "Processing ca_ca28.txt (211 of 500 files)\n",
            "Processing cf_cf47.txt (212 of 500 files)\n",
            "Processing cb_cb17.txt (213 of 500 files)\n",
            "Processing cn_cn07.txt (214 of 500 files)\n",
            "Processing ca_ca13.txt (215 of 500 files)\n",
            "Processing cd_cd10.txt (216 of 500 files)\n",
            "Processing cj_cj72.txt (217 of 500 files)\n",
            "Processing cj_cj17.txt (218 of 500 files)\n",
            "Processing cg_cg68.txt (219 of 500 files)\n",
            "Processing cg_cg13.txt (220 of 500 files)\n",
            "Processing ca_ca42.txt (221 of 500 files)\n",
            "Processing cg_cg08.txt (222 of 500 files)\n",
            "Processing cn_cn28.txt (223 of 500 files)\n",
            "Processing cg_cg15.txt (224 of 500 files)\n",
            "Processing cd_cd17.txt (225 of 500 files)\n",
            "Processing cg_cg56.txt (226 of 500 files)\n",
            "Processing ca_ca06.txt (227 of 500 files)\n",
            "Processing cf_cf03.txt (228 of 500 files)\n",
            "Processing cj_cj69.txt (229 of 500 files)\n",
            "Processing cg_cg35.txt (230 of 500 files)\n",
            "Processing ca_ca19.txt (231 of 500 files)\n",
            "Processing ch_ch21.txt (232 of 500 files)\n",
            "Processing cn_cn12.txt (233 of 500 files)\n",
            "Processing ce_ce11.txt (234 of 500 files)\n",
            "Processing cr_cr03.txt (235 of 500 files)\n",
            "Processing cf_cf16.txt (236 of 500 files)\n",
            "Processing cl_cl04.txt (237 of 500 files)\n",
            "Processing ce_ce01.txt (238 of 500 files)\n",
            "Processing cg_cg28.txt (239 of 500 files)\n",
            "Processing cm_cm01.txt (240 of 500 files)\n",
            "Processing cj_cj38.txt (241 of 500 files)\n",
            "Processing ck_ck16.txt (242 of 500 files)\n",
            "Processing cj_cj66.txt (243 of 500 files)\n",
            "Processing ca_ca12.txt (244 of 500 files)\n",
            "Processing ca_ca20.txt (245 of 500 files)\n",
            "Processing cl_cl02.txt (246 of 500 files)\n",
            "Processing ce_ce23.txt (247 of 500 files)\n",
            "Processing cg_cg36.txt (248 of 500 files)\n",
            "Processing cg_cg27.txt (249 of 500 files)\n",
            "Processing cf_cf37.txt (250 of 500 files)\n",
            "Processing cn_cn23.txt (251 of 500 files)\n",
            "Processing cg_cg12.txt (252 of 500 files)\n",
            "Processing cl_cl01.txt (253 of 500 files)\n",
            "Processing cl_cl12.txt (254 of 500 files)\n",
            "Processing ch_ch05.txt (255 of 500 files)\n",
            "Processing cj_cj01.txt (256 of 500 files)\n",
            "Processing ca_ca05.txt (257 of 500 files)\n",
            "Processing cj_cj26.txt (258 of 500 files)\n",
            "Processing cg_cg45.txt (259 of 500 files)\n",
            "Processing cc_cc02.txt (260 of 500 files)\n",
            "Processing cf_cf36.txt (261 of 500 files)\n",
            "Processing cg_cg57.txt (262 of 500 files)\n",
            "Processing cj_cj30.txt (263 of 500 files)\n",
            "Processing cb_cb01.txt (264 of 500 files)\n",
            "Processing ck_ck18.txt (265 of 500 files)\n",
            "Processing cj_cj64.txt (266 of 500 files)\n",
            "Processing cf_cf10.txt (267 of 500 files)\n",
            "Processing cc_cc14.txt (268 of 500 files)\n",
            "Processing cf_cf38.txt (269 of 500 files)\n",
            "Processing cg_cg47.txt (270 of 500 files)\n",
            "Processing cj_cj50.txt (271 of 500 files)\n",
            "Processing cn_cn14.txt (272 of 500 files)\n",
            "Processing ch_ch17.txt (273 of 500 files)\n",
            "Processing cf_cf44.txt (274 of 500 files)\n",
            "Processing cf_cf24.txt (275 of 500 files)\n",
            "Processing cp_cp10.txt (276 of 500 files)\n",
            "Processing cj_cj33.txt (277 of 500 files)\n",
            "Processing cp_cp02.txt (278 of 500 files)\n",
            "Processing ce_ce14.txt (279 of 500 files)\n",
            "Processing cg_cg53.txt (280 of 500 files)\n",
            "Processing cg_cg19.txt (281 of 500 files)\n",
            "Processing cg_cg49.txt (282 of 500 files)\n",
            "Processing cp_cp20.txt (283 of 500 files)\n",
            "Processing cj_cj51.txt (284 of 500 files)\n",
            "Processing cc_cc17.txt (285 of 500 files)\n",
            "Processing cg_cg11.txt (286 of 500 files)\n",
            "Processing ce_ce25.txt (287 of 500 files)\n",
            "Processing ca_ca16.txt (288 of 500 files)\n",
            "Processing cl_cl24.txt (289 of 500 files)\n",
            "Processing cj_cj19.txt (290 of 500 files)\n",
            "Processing cg_cg03.txt (291 of 500 files)\n",
            "Processing cj_cj21.txt (292 of 500 files)\n",
            "Processing cp_cp13.txt (293 of 500 files)\n",
            "Processing cp_cp05.txt (294 of 500 files)\n",
            "Processing cf_cf21.txt (295 of 500 files)\n",
            "Processing cc_cc09.txt (296 of 500 files)\n",
            "Processing cf_cf02.txt (297 of 500 files)\n",
            "Processing cg_cg22.txt (298 of 500 files)\n",
            "Processing cn_cn18.txt (299 of 500 files)\n",
            "Processing cg_cg43.txt (300 of 500 files)\n",
            "Processing ch_ch23.txt (301 of 500 files)\n",
            "Processing cn_cn06.txt (302 of 500 files)\n",
            "Processing cg_cg71.txt (303 of 500 files)\n",
            "Processing ce_ce29.txt (304 of 500 files)\n",
            "Processing cr_cr07.txt (305 of 500 files)\n",
            "Processing cf_cf48.txt (306 of 500 files)\n",
            "Processing cg_cg60.txt (307 of 500 files)\n",
            "Processing cp_cp07.txt (308 of 500 files)\n",
            "Processing ce_ce18.txt (309 of 500 files)\n",
            "Processing cj_cj35.txt (310 of 500 files)\n",
            "Processing cf_cf35.txt (311 of 500 files)\n",
            "Processing ce_ce20.txt (312 of 500 files)\n",
            "Processing cl_cl15.txt (313 of 500 files)\n",
            "Processing cc_cc04.txt (314 of 500 files)\n",
            "Processing cp_cp12.txt (315 of 500 files)\n",
            "Processing cg_cg14.txt (316 of 500 files)\n",
            "Processing cf_cf17.txt (317 of 500 files)\n",
            "Processing cb_cb19.txt (318 of 500 files)\n",
            "Processing cb_cb24.txt (319 of 500 files)\n",
            "Processing cg_cg72.txt (320 of 500 files)\n",
            "Processing cl_cl19.txt (321 of 500 files)\n",
            "Processing cn_cn01.txt (322 of 500 files)\n",
            "Processing cg_cg26.txt (323 of 500 files)\n",
            "Processing ck_ck29.txt (324 of 500 files)\n",
            "Processing cj_cj16.txt (325 of 500 files)\n",
            "Processing ck_ck20.txt (326 of 500 files)\n",
            "Processing ce_ce07.txt (327 of 500 files)\n",
            "Processing cj_cj04.txt (328 of 500 files)\n",
            "Processing ck_ck08.txt (329 of 500 files)\n",
            "Processing cg_cg54.txt (330 of 500 files)\n",
            "Processing ce_ce04.txt (331 of 500 files)\n",
            "Processing ck_ck14.txt (332 of 500 files)\n",
            "Processing cj_cj15.txt (333 of 500 files)\n",
            "Processing cf_cf08.txt (334 of 500 files)\n",
            "Processing ca_ca32.txt (335 of 500 files)\n",
            "Processing ce_ce19.txt (336 of 500 files)\n",
            "Processing cd_cd06.txt (337 of 500 files)\n",
            "Processing cp_cp22.txt (338 of 500 files)\n",
            "Processing cg_cg44.txt (339 of 500 files)\n",
            "Processing cb_cb14.txt (340 of 500 files)\n",
            "Processing ca_ca08.txt (341 of 500 files)\n",
            "Processing cj_cj25.txt (342 of 500 files)\n",
            "Processing cb_cb04.txt (343 of 500 files)\n",
            "Processing ce_ce16.txt (344 of 500 files)\n",
            "Processing cp_cp08.txt (345 of 500 files)\n",
            "Processing cn_cn16.txt (346 of 500 files)\n",
            "Processing ce_ce26.txt (347 of 500 files)\n",
            "Processing cc_cc12.txt (348 of 500 files)\n",
            "Processing ce_ce30.txt (349 of 500 files)\n",
            "Processing cn_cn10.txt (350 of 500 files)\n",
            "Processing ch_ch12.txt (351 of 500 files)\n",
            "Processing cb_cb23.txt (352 of 500 files)\n",
            "Processing ce_ce27.txt (353 of 500 files)\n",
            "Processing cm_cm03.txt (354 of 500 files)\n",
            "Processing cg_cg06.txt (355 of 500 files)\n",
            "Processing cg_cg65.txt (356 of 500 files)\n",
            "Processing cd_cd07.txt (357 of 500 files)\n",
            "Processing ch_ch19.txt (358 of 500 files)\n",
            "Processing cb_cb15.txt (359 of 500 files)\n",
            "Processing ce_ce13.txt (360 of 500 files)\n",
            "Processing ck_ck19.txt (361 of 500 files)\n",
            "Processing ca_ca22.txt (362 of 500 files)\n",
            "Processing cj_cj49.txt (363 of 500 files)\n",
            "Processing cd_cd01.txt (364 of 500 files)\n",
            "Processing cc_cc11.txt (365 of 500 files)\n",
            "Processing cj_cj75.txt (366 of 500 files)\n",
            "Processing cg_cg32.txt (367 of 500 files)\n",
            "Processing ch_ch02.txt (368 of 500 files)\n",
            "Processing cj_cj13.txt (369 of 500 files)\n",
            "Processing cn_cn03.txt (370 of 500 files)\n",
            "Processing cb_cb22.txt (371 of 500 files)\n",
            "Processing cp_cp03.txt (372 of 500 files)\n",
            "Processing cf_cf26.txt (373 of 500 files)\n",
            "Processing ck_ck12.txt (374 of 500 files)\n",
            "Processing cr_cr09.txt (375 of 500 files)\n",
            "Processing cj_cj20.txt (376 of 500 files)\n",
            "Processing cm_cm02.txt (377 of 500 files)\n",
            "Processing cg_cg33.txt (378 of 500 files)\n",
            "Processing cc_cc10.txt (379 of 500 files)\n",
            "Processing ce_ce21.txt (380 of 500 files)\n",
            "Processing cj_cj02.txt (381 of 500 files)\n",
            "Processing cg_cg07.txt (382 of 500 files)\n",
            "Processing ca_ca37.txt (383 of 500 files)\n",
            "Processing cr_cr06.txt (384 of 500 files)\n",
            "Processing cb_cb12.txt (385 of 500 files)\n",
            "Processing cj_cj79.txt (386 of 500 files)\n",
            "Processing cj_cj65.txt (387 of 500 files)\n",
            "Processing cf_cf45.txt (388 of 500 files)\n",
            "Processing ch_ch13.txt (389 of 500 files)\n",
            "Processing cj_cj71.txt (390 of 500 files)\n",
            "Processing cg_cg64.txt (391 of 500 files)\n",
            "Processing cf_cf32.txt (392 of 500 files)\n",
            "Processing cg_cg66.txt (393 of 500 files)\n",
            "Processing cf_cf27.txt (394 of 500 files)\n",
            "Processing cg_cg40.txt (395 of 500 files)\n",
            "Processing ce_ce28.txt (396 of 500 files)\n",
            "Processing cr_cr02.txt (397 of 500 files)\n",
            "Processing cj_cj47.txt (398 of 500 files)\n",
            "Processing ck_ck07.txt (399 of 500 files)\n",
            "Processing ca_ca25.txt (400 of 500 files)\n",
            "Processing ce_ce36.txt (401 of 500 files)\n",
            "Processing cj_cj74.txt (402 of 500 files)\n",
            "Processing cf_cf15.txt (403 of 500 files)\n",
            "Processing cf_cf04.txt (404 of 500 files)\n",
            "Processing cg_cg37.txt (405 of 500 files)\n",
            "Processing cd_cd13.txt (406 of 500 files)\n",
            "Processing ck_ck27.txt (407 of 500 files)\n",
            "Processing cp_cp19.txt (408 of 500 files)\n",
            "Processing cl_cl05.txt (409 of 500 files)\n",
            "Processing cn_cn02.txt (410 of 500 files)\n",
            "Processing cj_cj31.txt (411 of 500 files)\n",
            "Processing ck_ck02.txt (412 of 500 files)\n",
            "Processing ce_ce35.txt (413 of 500 files)\n",
            "Processing ch_ch09.txt (414 of 500 files)\n",
            "Processing cg_cg34.txt (415 of 500 files)\n",
            "Processing cm_cm06.txt (416 of 500 files)\n",
            "Processing ch_ch29.txt (417 of 500 files)\n",
            "Processing cb_cb07.txt (418 of 500 files)\n",
            "Processing ca_ca27.txt (419 of 500 files)\n",
            "Processing cj_cj57.txt (420 of 500 files)\n",
            "Processing ck_ck06.txt (421 of 500 files)\n",
            "Processing ch_ch20.txt (422 of 500 files)\n",
            "Processing cg_cg67.txt (423 of 500 files)\n",
            "Processing ch_ch30.txt (424 of 500 files)\n",
            "Processing cl_cl13.txt (425 of 500 files)\n",
            "Processing cf_cf25.txt (426 of 500 files)\n",
            "Processing cn_cn19.txt (427 of 500 files)\n",
            "Processing cg_cg46.txt (428 of 500 files)\n",
            "Processing cg_cg04.txt (429 of 500 files)\n",
            "Processing cf_cf41.txt (430 of 500 files)\n",
            "Processing cl_cl14.txt (431 of 500 files)\n",
            "Processing cj_cj18.txt (432 of 500 files)\n",
            "Processing cg_cg25.txt (433 of 500 files)\n",
            "Processing cl_cl21.txt (434 of 500 files)\n",
            "Processing cn_cn26.txt (435 of 500 files)\n",
            "Processing cj_cj59.txt (436 of 500 files)\n",
            "Processing ch_ch04.txt (437 of 500 files)\n",
            "Processing cg_cg51.txt (438 of 500 files)\n",
            "Processing cf_cf12.txt (439 of 500 files)\n",
            "Processing cj_cj22.txt (440 of 500 files)\n",
            "Processing cp_cp14.txt (441 of 500 files)\n",
            "Processing ck_ck24.txt (442 of 500 files)\n",
            "Processing cj_cj39.txt (443 of 500 files)\n",
            "Processing cg_cg48.txt (444 of 500 files)\n",
            "Processing cj_cj43.txt (445 of 500 files)\n",
            "Processing cn_cn04.txt (446 of 500 files)\n",
            "Processing cl_cl10.txt (447 of 500 files)\n",
            "Processing cg_cg20.txt (448 of 500 files)\n",
            "Processing ck_ck03.txt (449 of 500 files)\n",
            "Processing cg_cg21.txt (450 of 500 files)\n",
            "Processing ch_ch28.txt (451 of 500 files)\n",
            "Processing cg_cg69.txt (452 of 500 files)\n",
            "Processing cj_cj11.txt (453 of 500 files)\n",
            "Processing cg_cg75.txt (454 of 500 files)\n",
            "Processing cb_cb20.txt (455 of 500 files)\n",
            "Processing cp_cp17.txt (456 of 500 files)\n",
            "Processing ca_ca21.txt (457 of 500 files)\n",
            "Processing ca_ca40.txt (458 of 500 files)\n",
            "Processing ck_ck11.txt (459 of 500 files)\n",
            "Processing cc_cc06.txt (460 of 500 files)\n",
            "Processing cn_cn20.txt (461 of 500 files)\n",
            "Processing cj_cj09.txt (462 of 500 files)\n",
            "Processing ck_ck22.txt (463 of 500 files)\n",
            "Processing cj_cj48.txt (464 of 500 files)\n",
            "Processing ca_ca44.txt (465 of 500 files)\n",
            "Processing ce_ce22.txt (466 of 500 files)\n",
            "Processing ce_ce34.txt (467 of 500 files)\n",
            "Processing cj_cj56.txt (468 of 500 files)\n",
            "Processing cf_cf22.txt (469 of 500 files)\n",
            "Processing cb_cb26.txt (470 of 500 files)\n",
            "Processing ce_ce09.txt (471 of 500 files)\n",
            "Processing cr_cr01.txt (472 of 500 files)\n",
            "Processing cj_cj78.txt (473 of 500 files)\n",
            "Processing cl_cl09.txt (474 of 500 files)\n",
            "Processing ca_ca02.txt (475 of 500 files)\n",
            "Processing cl_cl16.txt (476 of 500 files)\n",
            "Processing ch_ch08.txt (477 of 500 files)\n",
            "Processing cb_cb27.txt (478 of 500 files)\n",
            "Processing ck_ck15.txt (479 of 500 files)\n",
            "Processing cj_cj29.txt (480 of 500 files)\n",
            "Processing cj_cj63.txt (481 of 500 files)\n",
            "Processing cb_cb06.txt (482 of 500 files)\n",
            "Processing cp_cp06.txt (483 of 500 files)\n",
            "Processing ce_ce15.txt (484 of 500 files)\n",
            "Processing cg_cg30.txt (485 of 500 files)\n",
            "Processing cb_cb11.txt (486 of 500 files)\n",
            "Processing cg_cg42.txt (487 of 500 files)\n",
            "Processing cj_cj76.txt (488 of 500 files)\n",
            "Processing cn_cn27.txt (489 of 500 files)\n",
            "Processing cc_cc08.txt (490 of 500 files)\n",
            "Processing cn_cn24.txt (491 of 500 files)\n",
            "Processing cb_cb08.txt (492 of 500 files)\n",
            "Processing cj_cj37.txt (493 of 500 files)\n",
            "Processing ch_ch16.txt (494 of 500 files)\n",
            "Processing cj_cj36.txt (495 of 500 files)\n",
            "Processing ck_ck23.txt (496 of 500 files)\n",
            "Processing ca_ca23.txt (497 of 500 files)\n",
            "Processing cf_cf01.txt (498 of 500 files)\n",
            "Processing cp_cp24.txt (499 of 500 files)\n",
            "Processing ce_ce05.txt (500 of 500 files)\n",
            "Search returned 524 hits.\n",
            " Returning a random sample of 10 hits\n",
            "[['see', 'her', 'settled', 'on', 'my', 'way', 'to', 'the', 'elevator', 'i'], 'ran', ['into', 'pete', 'i', 've', 'got', 'the', 'results', 'on', 'the', 'bancroft']]\n",
            "[['bases', 'loaded', 'and', 'in', 'unprecedented', 'style', 'delivered', 'consecutive', 'grand-slam', 'home'], 'runs', ['2', 'willie', 'mays', 'of', 'the', 'san', 'francisco', 'giants', 'borrowed', 'a']]\n",
            "[['scuse', 'me', 'sir', 'let', 's', 'get', 'out', 'of', 'here', 'watson'], 'ran', ['up', 'the', 'ladder', 'and', 'stood', 'for', 'a', 'second', 'sucking', 'in']]\n",
            "[['found', 'were', 'inadequate', 'many', 'of', 'them', 'in', 'increasing', 'panic', 'came'], 'running', ['with', 'water', 'in', 'their', 'hats', 'in', 'a', 'ludicrous', 'effort', 'both']]\n",
            "[['nineties', 'each', 'film', 'consisted', 'of', 'fifty', 'feet', 'which', 'gives', 'a'], 'running', ['time', 'of', 'about', 'one', 'minute', 'on', 'the', 'screen', 'as', 'long']]\n",
            "[['burning', 'on', 'its', 'under', 'side', 'but', 'not', 'visibly', 'a', 'ripple'], 'ran', ['through', 'the', 'muscles', 'of', 'his', 'jaws', 'but', 'he', 'kept', 'control']]\n",
            "[['vienna', 'established', 'ss', 'headquarters', 'for', 'all', 'of', 'poland', 'the', 'bathyrans'], 'ran', ['a', 'check', 'on', 'globocnik', 'and', 'had', 'only', 'to', 'conclude', 'that']]\n",
            "[['engisch', 'was', 'waiting', 'there', 'on', 'the', 'steps', 'and', 'she', 'came'], 'running', ['toward', 'us', 'she', 's', 'nowhere', 'nowhere', 'she', 'screamed', 'and', 'both']]\n",
            "[['success', 'stories', 'need', 'to', 'be', 'heavily', 'publicized', 'here', 'again', 'we'], 'run', ['into', 'the', 'roadblock', 'that', 'negroes', 'do', 'not', 'like', 'to', 'be']]\n",
            "[['long', 'as', 'they', 'need', 'him', 'we', 'cant', 'touch', 'him', 'they'], 'run', ['their', 'show', 'their', 'way', 'the', 'universe', 'has', 'variety', 'something', 'for']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. 단어 용례 분석 때 문맥에 나타나는 단어에 특정 단어가 함께 나타나는지 확인해 볼 수 있다. 이를 위해서 Concordance (단어 용례) 함수에 Collocation (단어 동반 출현) 검색 인자를 추가한다.  \n",
        "- 동반 출현하는지 확인하고 싶은 단어는 \" \" 안에 넣어 문자열의 형태로 리스트 [ ] 안에 쉼표를 사용하여 넣고 이를 collocates 변수에 할당한다 (예시: collocates = [\"quick\",\"quickly\"]). \n",
        "\n",
        "[코드분석 1] 파일 읽기 -> 토큰화 -> 단어 리스트에 포함된 [\"run\", \"ran\", \"running\", \"runs\"] 의 용법을 알아볼 수 있는 함께 나오는 단어 리스트에 동반 출현하는 [\"quick\", \"quickly\"] 가 포함된 결과를  무작위로 10 개 뽑는다. 이를 con_results2 변수에 할당한다."
      ],
      "metadata": {
        "id": "5abMHLdAErmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results2 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"],collocates = [\"quick\",\"quickly\"], nhits = 10)\n",
        "for x in conc_results2:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "id": "AZRzHbwUnjgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. 분석 대상이 되는 단어를 리스트에 요인으로 넣을 때 정규표현식을 사용하여 더 간단하고, 범위가 넓게 검색할 수 있다.  \n",
        "- 분석 대상 단어 표현에 정규표현식을 포함함과 동시에 Concordance ( ) 함수에 마지막 인자에 regex = True 를 추가한다. Collocation (단어 동반 출현) 검색 인자를 추가한다 (예시: [\"run.*\", \"ran\"]... regex = True) \n",
        "- Regular Expression 기호 해설\n",
        "-- 임의의 문자 (.) \n",
        "-- 문자수가 0~무한대 (*)\n",
        "--Q: 임의의 문자 (.)을 넣지 않고 (*)만 추가했을 때에는 분석 대상 단어에 세번째 철자가 n 이 아닌 \"rulers, ruffles, russia, ruger\" 등등의 단어도 검색되어 결과로 나옴. 왜요??? "
      ],
      "metadata": {
        "id": "tWn3vaXEQ_YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results3 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run.*\",\"ran\"],collocates = [\"quick.*\"], nhits = 10, regex = True)\n",
        "for x in conc_results3:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "id": "Bf2gOwCnoDCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. 검색 결과를 새파일로 저장하기 (예시: \"run_25.txt\")\n",
        "\n",
        "- concordance 코드로 출력한 결과를 새파일로 저장할 때에는, concord( ) 함수 마지막에 새로 저장할 파일 이름을 outcome 변수를 사용하여 지정해 준다 (예시: outname = \"run_25.txt\").\n",
        "\n",
        "- 전체 결과를 저장하기 위해서는 \"nhits = 숫자\" 인자는 포함 안한다."
      ],
      "metadata": {
        "id": "JWs-XSoKpr1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write concordance lines to a file called \"run_25.txt\"\n",
        "conc_results4 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"], outname = \"run_25.txt\")"
      ],
      "metadata": {
        "id": "MCU4o-D4oQo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpus_Toolkit 사용하여 Tagging (단어에 품사를 연결)\n",
        "\n",
        "###1. 단어에 품사를 연결한 결과를 새파일로 저장하기 \n",
        "\n",
        "- [코드해석 1] 파일을 읽어 불러들여들여 태그하여 tagged_brown 변수에 할당한다.\n",
        "\n",
        "- [코드해석 2]  tagged_brown 변수 내용이 들어가 있는 tagged_brown_single 새파일을 만든다.   이 때 ct 모듈의 write_corpus (\"A\", B ) 함수를 사용하는데, 첫 번 째 인자 A 는 \" \" 안에 새파일이름을 넣고, 두 번 째 인자 B 는 파일 안에 들어가는 변수를 넣는다.\n",
        "\n",
        "- [코드해석 3] Nest 기법으로 위의 파일 불러들여 읽고 태그하기 [코드 1]과 태그한 결과를 새파일로 만들기 [코드 2]를 한 줄에 작성"
      ],
      "metadata": {
        "id": "NN3_IsC5W6ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_brown = ct.tag(ct.ldcorpus(\"brown_single\"))\n",
        "ct.write_corpus(\"tagged_brown_single\",tagged_brown) #the first argument is the folder where the tagged files will be written"
      ],
      "metadata": {
        "id": "q_HExln_qlMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ct.write_corpus(\"tagged_brown_single\",ct.tag(ct.ldcorpus(\"brown_single\")))"
      ],
      "metadata": {
        "id": "ubYWE2zDsvYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. 텍스트에 나오는 단어에 픔사를 연결한 것에서 품사의 빈도수를 조사하기\n",
        "\n",
        "- 1. [코드해석 1] 태깅한 단어-품사를 저장한 파일을 다시 불러들이고 품사 빈도수를 계산하여 이를 tagged_freq 변수에 할당한다. frequency ( ) 함수를 사용한다.  \n",
        "\n",
        "- 2. [코드해석 2] tagged_freq 번수에 담긴 결과에서 빈도수가 가장 높은 10 개를 결과로 출력한다. head (A, B) 함수를 사용하고, 두 번 째 B 인자로 hits = 10 을 지정해 준다. "
      ],
      "metadata": {
        "id": "rQSMc4jTe_gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_freq = ct.frequency(ct.reload(\"tagged_brown_single\"))\n",
        "ct.head(tagged_freq, hits = 10)"
      ],
      "metadata": {
        "id": "syhi8MT0s7xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collocate (단어 동반 출현) \n",
        "####[Collocate 이해에 도움되는 문서](\"https://hecc.ubc.ca/quantitative-textual-analysis/qta-practice/what-antconcs-concordance-tool-can-do-for-you/\")\n",
        "\n",
        "특정 단어와 단어 동반 출현하는 단어의 빈도 지수를 분석 후 상위 10 개 결과 출력\n",
        "- [코드해석 1] ct 모듈 collocator (A, B, C) 함수를 사용\n",
        "-- A 인자: 토큰화된 단어 (예시: tokenize ( ) 함수)\n",
        "-- B 인자: \"대상 단어\" (예시: \"go\")\n",
        "-- C 인자: Sort by (정렬방식). 기본형은 stat. stat 의 기본형은 MI (mutual information (상호정보)) (예시: stat = \"MI\")\n",
        "-- C 인자 추가설명: The default “Sort By” is “Stat”. And the default test statistic is MI, or Mutual Information. Mutual Information represents a ratio of the observed frequency (fo) of the combination of two words (or two word phrases) divided by the expected frequency (fe) of the combination: fo / fe .\n",
        "- [코드해석 2] ct 모듈 head (A, B) 함수 사용\n",
        "-- A 인자: 동반단어결과 변수\n",
        "-- B 인자: hits = 숫자\n",
        "\n"
      ],
      "metadata": {
        "id": "BtrKQoRHskKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collocates = ct.collocator(ct.tokenize(ct.ldcorpus(\"brown_single\")),\"go\",stat = \"MI\")\n",
        "#stat options include: \"MI\", \"T\", \"freq\", \"left\", and \"right\"\n",
        "\n",
        "ct.head(collocates, hits = 10)"
      ],
      "metadata": {
        "id": "TWCovRiTtJtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#First, generate frequency lists for each corpus\n",
        "corp1freq = ct.frequency(ct.tokenize(ct.ldcorpus(\"corp1\")))\n",
        "corp2freq = ct.frequency(ct.tokenize(ct.ldcorpus(\"corp2\")))\n",
        "\n",
        "#then calculate Keyness\n",
        "corp_key = ct.keyness(corp1freq,corp2freq, effect = \"log-ratio\")\n",
        "ct.head(corp_key, hits = 10) #to display top hits\n"
      ],
      "metadata": {
        "id": "HcUhyymttUi9",
        "outputId": "4a572caa-f4b1-47a0-84d0-50233cf37266",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No files found. There may be a problem with your working directory or your file search term.\n",
            "No files found. There may be a problem with your working directory or your file search term.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-grams (N-그램)\n",
        "N-그램은 몇 개 단어가 이웃 하여 연속되어 나오는 것이다. tokenize ( ) 함수에서 ngram 인자를 포함하여 분석해서 이웃한 단어의 연속을 토큰화 한다. 각각의 토큰화된 n-그램에 대한 빈도수를 측정 후 상위 10 개 결과 출력 \n",
        "- [코드해석 1] ct 모듈 tokenize (A, B, C) 함수를 사용 \n",
        "-- A 인자: 분석할 대상 텍스트 불러들여 읽기\n",
        "-- B 인자: 표준 단어 사용 여부 (예시: lemma = False)\n",
        "-- C 인자: 이웃 단어 숫자 지정 (예시: ngram = 숫자): 숫자에 3을 넣으면 3 개의 이웃한 단어를 분석함  \n",
        "  \n",
        "- [코드해석 2] ct 모듈 frequency ( ) 함수를 사용하여 토큰화된 각각의 n-그램에 대한 빈도수를 계산한다. \n",
        "\n",
        "- [코드해석 3] ct 모듈 head (A, B) 함수 사용\n",
        "-- A 인자: 토큰화된 각각의 n-gram 빈도수 결과 변수\n",
        "-- B 인자: hits = 숫자 (예시: 10 개 결과 출력)\n",
        "\n",
        "[코드 1]과 [코드 2]를 Nest 기법으로 한 줄 명령도 가능\n"
      ],
      "metadata": {
        "id": "AC3sbfm_uRO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigramfreq = ct.frequency(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False, ngram = 3))\n",
        "ct.head(trigramfreq, hits = 10)"
      ],
      "metadata": {
        "id": "Ll9zKBO4uAo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dependency bigrams\n",
        "주요어-의존어 이웃하여 연속으로 오는 것이다. 통사적으로 Head (주요어)와 Dependency (의존어)는 연결되어 있다. \"The player kicked the ball.\" 문장에서 \"kicked\" 와 \"ball\"은 통사적으로 동사와 직접 목적어 관계에 있고, 동사는 주요어, 목적어는 의존어라고 본다. dep_bigram ( ) 함수는 문장을 범위로 하여 주요어-의존어를 찾는다. \n",
        "\n",
        "dep_bigram ( ) 함수를 사용하여 주요어-의존어 관계를 찾고, 이 단어 쌍들의 빈도수를 계산하며 상위 10 개 결과 출력\n",
        "- [코드해석 1] ct 모듈 dep_bigram (A,B) 함수 사용\n",
        "-- A 인자: 분석할 대상 텍스트를 불러들여 읽기\n",
        "-- B 인자: 의존어 지정. direct object (직접목적어)를 인자로 사용할 때는 \" \" 안에 dobj 를 넣는다. (예시: \"dobj\") \n",
        "- [코드해석 2] ct 모듈 head (A, B) 함수 사용\n",
        "-- A 인자: 주요어-의존어 변수에 [\"bi_freq\"]를 써서 해당 변수의 빈도수 계산을 가능하게 한다.\n",
        "-- B 인자: hits = 숫자 (예시: 10 개 결과 출력)\n",
        "\n"
      ],
      "metadata": {
        "id": "js4OUwWuubZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bg_dict = ct.dep_bigram(ct.ldcorpus(\"brown_single\"),\"dobj\")\n",
        "ct.head(bg_dict[\"bi_freq\"], hits = 10)\n",
        "#other keys include \"dep_freq\", \"head_freq\", and \"range\"\n",
        "#also note that the key \"samples\" can be used to obtain a list of sample sentences\n",
        "#but, this is not compatible with the ct.head() function (see ct.dep_conc() instead)"
      ],
      "metadata": {
        "id": "BJy-ilc-uf4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Strength of association (연결강도)\n",
        "주요어-의존어에 대하여 두 단어의 연결강도를 측정할 수 있다. \n",
        "\n",
        "soa( ) 함수는 dep_bigram ( ) 함수 결과 (즉, 주요어-의존어를 쌍으로 출력)를 사용하여 각각의 두 단어 쌍간에 연결강도를 계산한다.    \n",
        "- [코드해석 1] ct 모듈 soa (A, B) 함수\n",
        "-- A 인자: dep_bigram ( ) 함수 결과 변수\n",
        "-- B 인자: Sort by (정렬방식). 기본형은 stat. stat 의 기본형은 MI (mutual information (상호정보)) (예시: stat = \"MI\")\n",
        "- [코드해석 2] ct 모듈 head (A, B) 함수 사용\n",
        "-- A 인자: 사전형태로 출력된 주요어-의존어 사이의 연결강도 변수\n",
        "-- B 인자: hits = 숫자 (예시: 10 개 결과 출력)\n"
      ],
      "metadata": {
        "id": "UewcMQUOuxRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soa_mi = ct.soa(bg_dict,stat = \"MI\")\n",
        "#other stat options include: \"T\", \"faith_dep\", \"faith_head\",\"dp_dep\", and \"dp_head\"\n",
        "ct.head(soa_mi, hits = 10)"
      ],
      "metadata": {
        "id": "CNvuN00Cu1Da",
        "outputId": "d62bac39-40c6-4a14-d98f-7baae02097aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class_judge\t10.920684367813834\n",
            "nose_scratch\t10.715966457570863\n",
            "suicide_commit\t10.378096818814479\n",
            "nose_blow\t10.171645941347052\n",
            "imagination_capture\t9.963753089705719\n",
            "calendar_adjust\t9.908611535513257\n",
            "English_speak\t9.323649034792101\n",
            "resemblance_bear\t9.253259706900703\n",
            "throat_clear\t9.1501662139366\n",
            "expense_deduct\t9.1501662139366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LD_-BqK0Nlax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###dep_conc ( ) 함수는 dep_bigram ( ) 함수 결과 (즉, 주요어-의존어를 쌍으로 출력)를 사용하여 각각의 두 단어 쌍의 용례 내용이 들어 있는 새파일을 현재 작업 디렉토리에 생성한다. \n",
        "\n",
        "- [코드해석 1] ct 모듈 dep_conc (A, B) 함수\n",
        "-- A 인자: dep_bigram ( ) 함수 결과 변수 옆에 [\"samples\"]를 쓰면 기본값으로 50 개 샘플이 무작위로 생성\n",
        "-- B 인자: \" \" 안에 dobj_results.html 넣어 새파일명과 포맷을 지정 \n",
        "\n",
        "단어 용법을 분석할 수 있는 AntConc 는 GUI (그래픽 사용자 인터페이스)를 기반으로 하고 사용자 선호도가 높고 무료로 이용 가능하다.\n",
        "[AntConc 사용 문학 분석 예시](\"https://hecc.ubc.ca/quantitative-textual-analysis/qta-practice/what-antconcs-concordance-tool-can-do-for-you/\")\n",
        "\n",
        "Q: html 파일을 웹상에서 어떻게 열어 보나요? "
      ],
      "metadata": {
        "id": "G9iry7cpvEbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ct.dep_conc(bg_dict[\"samples\"],\"dobj_results\")"
      ],
      "metadata": {
        "id": "Nxh0aFGou9cP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}