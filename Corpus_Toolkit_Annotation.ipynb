{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM8vndpIjzQC0uEhECazYVs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms624atyale/SPR2022/blob/main/Corpus_Toolkit_Annotation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Corpus (코퍼스/말뭉치) \n",
        "- 코퍼스는 언어의 본질을 보여줄 수 있도록 문장 자료를 모아놓은 것이다. 코퍼스로 분석할 수 있는 것들은 i) 단어수 계산, ii) n-gram (코퍼스에서 n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 취급) 빈도수와 범워, iii) 키값, iv) collocation (연어), iv) 의존합자식별(예: 동사-직접목적어 조합) 및 의존합자식별 빈도-범위-연결강도 등이 있다.  \n",
        "\n",
        "- 위의 분석을 위해서는 corpus-toolkit 패키지를 설치하거나 colab에 이미 기본적으로 설치되어 있는 패키지에서 분석에 필요한 내부 함수를 적절하게 코드 작성 때 사용한다.   \n",
        "-- corpus-toolkit 패키지의 tokenization(토큰화) & lemmatization(표준형태화) 사용\n",
        "-- colab에 이미 설치되어 있는 tagging(단어에 문법범주 연결) & parsing(구분자를 사용하여 문자열을 구성 요소로 분석) 사용"
      ],
      "metadata": {
        "id": "M7afPLwh-bRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Colab 작업 디렉토리에서 폴더를 생성하여 다른 곳에서 다운로드 받은 단/복수의 파일을 옮겨 넣는 방법 \n",
        "- [새폴더 만들기] 코랩에 기본으로 깔려 있는 sample_data 에 커서를 대면 활성화되면서 우측에 세로 점 3 개가 나타난다. 거기를 클릭하면 선택사항 -> new folder를 클릭한다. \n",
        "- [폴더를 샘플 디렉토리와 같은 레벨로 이동] new folder에 커서를 대면 활성화되면서 우측에 세로 점 3 개가 나타난다. 해당 폴더에 커서를 대고 누르고 있으면 하단에 <Drag file and ...> 창이 나타나는데, 여기로 해당 폴더를 끌어다 넣는다.\n",
        "- [폴더 새이름]해당 폴더에 Rename folder 클릭해서 파일명을 새이름으로 바꾼다. 예)brown_single\n",
        "- [폴더에 파일 업로드] 해당 폴더에 커서를 대면 활성화되면서 우측에 세로 점 3 개가 나타난다. Upload 클릭해서 본인 컴퓨터에 다운로드 받은 복수의 파일을 (혹은 기존 파일) 모두 업로드 한다. "
      ],
      "metadata": {
        "id": "IpAwLbK32qeN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBz4z96Z-YV5"
      },
      "outputs": [],
      "source": [
        "pip install corpus-toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Pre-processing (사전처리)\n",
        "-- 코퍼스 분석을 하려면 분석하려는 텍스트의 단어를 토큰화하여 분석할 수 있는 형태로 준비해 놓아야 한다. i) tokenize 함수를 사용하여 토큰을 리스트화 하여 준비해 놓을 수도 있고, ii) tag 함수를 사용하여 토큰에 문법범주(품사)를 연결한 형태로 준비해 놓을 수도 있다. \n",
        "\n",
        "#####[코드문법1: from~ import ~ as ~] \n",
        "- corpus_toolkit패키지에서 corpus_tools모듈을 ct로 줄여서 불러들여라.\n",
        "\n",
        "#####[코드문법2: 파일 올리고 읽기] ldcorpus( )는 파일 읽기 \n",
        "- ct모듈의 ldcorpus( )함수에 brwon_single 데이터 폴더를 인자로 넣어, 그 결과를 brown_corp변수에 할당한다.\n",
        "\n",
        "#####[코드문법3: 단어 토큰화] tokenize( )는 데이터 토큰화\n",
        "- ct모듈의 tokenize( )함수 인자로 brown_corp 변수를 넣어, 그 결과를 tok_corp변수에 할당한다.\n",
        "\n",
        "#####[코드문법4: 토큰화된 단어 빈도수 계산] : frequency( )는 토큰의 빈도수 계산\n",
        "- ct모듈의 frequency( )함수 인자로 tok_corp 변수를 넣어, 그 결과를 brown_freq변수에 할당한다.\n",
        "\n",
        "#####[코드문법5: 빈도수 높은 10 개 출력]\n",
        "- ct모듈의 head( )함수에 brown_freq변수를 첫번째 인자로, 빈도수가 높은 것 10 개를 두번째 인자로 사용해서 (hits = 10), 상위 10 개 토큰의 빈도수를 출력한다."
      ],
      "metadata": {
        "id": "bO38yLp90864"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from corpus_toolkit import corpus_tools as ct\n",
        "brown_corp = ct.ldcorpus(\"brown_single\") #load and read corpus\n",
        "tok_corp = ct.tokenize(brown_corp) #tokenize corpus - by default this lemmatizes as well\n",
        "brown_freq = ct.frequency(tok_corp) #creates a frequency dictionary\n",
        "##note that range can be calculated instead of frequency using the argument calc = \"range\"\n",
        "ct.head(brown_freq, hits = 10) #print top 10 items"
      ],
      "metadata": {
        "id": "17lsll3VIzu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Nest (차곡히 쌓기): 여러 줄의 명령어에 대하여 괄호를 계층적으로 사용하여 한 줄 명령어로 작성    \n",
        "위 명령창에 코드를 작성 때에는 \"모듈.함수( )\"가 한 세트인 명령어 사용하여 i) 파일을 로드해서 읽고 -> ii) 토큰화 하고 -> iii) 토큰의 빈도수를 계산하는 명령어를 사용하여 코드로 작성했다. 일련의 코드는 Nest 기법을 사용해서 한 줄 짜리 코드로 작성이 가능하다. 예시는 아래에 있다.  \n"
      ],
      "metadata": {
        "id": "R-98Zbi6j8qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown_freq = ct.frequency(ct.tokenize(ct.ldcorpus(\"brown_single\")))\n",
        "ct.head(brown_freq, hits = 10)"
      ],
      "metadata": {
        "id": "zxrWoaKbj-Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Concord \n",
        "\n",
        "### 5.1 Concordance (단어 용례 분석 or 색인?) \n",
        "텍스트에서 관심 있는 특정 단어가 어떤 맥락에서 사용되는 지 알기 위해서 함께 나오는 단어를  방법이 있다. 여기에 다른 단어를 추가하여 특정 단어가 나타난 환경을 더 세부적으로 분석할 수도 있다. \n",
        "\n",
        "### 5.2 Concordacne (단어 용법 분석 or 색인?)을 할 때에 세 가지를 지정해 준다. \n",
        "- (1) 맥락에서 함께 등장하는 단어를 출력할 때 텍스트에 나온 형태 그대로 사용하고자 할 때에는 (lemma=False)라고 지정\n",
        "- (2) 분석 대상이 되는 단어는 \" \" 안에 넣어 문자열의 형태로 리스트 [ ] 안에 쉼표를 사용해서 넣어 준다. (예시 [\"run\", \"ran\", 'running\", \"runs\"]\n",
        "- (3) 특정 단어는 그 앞에 10 단어와 그 뒤에 10 단어가 기본값으로 출력되는데, 이 때 보고 싶은 라인별 출력량은 nhits에 원하는 숫자를 쓰면 된다. 무작위로 생성되며 1 ~ N 으로 입력 가능하다.  \n",
        "\n",
        "[코드분석 1] 파일 읽기 -> 토큰화 -> 단어 리스트에 포함된 [\"run\", \"ran\", \"running\", \"runs\"] 의 용법을 알아볼 수 있는 함께 나오는 단어 뽑되 그 결과를 무작위로 10 개 뽑는다. 이를 con_results 변수에 할당한다. \n",
        "- ldcorpus ( ) 함수로 brwon_single 파일을 읽기\n",
        "- tokenize ( ) 함수로 텍스트 단어를 토큰화\n",
        "- concord ( ) 함수로 단어 용례 환경을 출력  \n",
        "- 결과를 한 개 씩 보기 좋게 출력하기 위하여 for 함수를 사용한다. con_results 변수에서 찾는 단어 나타날 때 마다 그 결과를 각각 출력한다.    "
      ],
      "metadata": {
        "id": "c7caiVBZO5d7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results1 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"],nhits = 10)\n",
        "for x in conc_results1:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "id": "_z3M-DivngQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.3 단어 용례 분석 때 문맥에 나타나는 단어에 특정 단어가 함께 나타나는지 확인해 볼 수 있다. 이를 위해서 Concordance (단어 용례) 함수에 Collocation (단어 동반 출현) 검색 인자를 추가한다.  \n",
        "- 동반 출현하는지 확인하고 싶은 단어는 \" \" 안에 넣어 문자열의 형태로 리스트 [ ] 안에 쉼표를 사용하여 넣고 이를 collocates 변수에 할당한다 (예시: collocates = [\"quick\",\"quickly\"]). \n",
        "\n",
        "[코드분석 1] 파일 읽기 -> 토큰화 -> 단어 리스트에 포함된 [\"run\", \"ran\", \"running\", \"runs\"] 의 용법을 알아볼 수 있는 함께 나오는 단어 리스트에 동반 출현하는 [\"quick\", \"quickly\"] 가 포함된 결과를  무작위로 10 개 뽑는다. 이를 con_results2 변수에 할당한다."
      ],
      "metadata": {
        "id": "5abMHLdAErmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results2 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"],collocates = [\"quick\",\"quickly\"], nhits = 10)\n",
        "for x in conc_results2:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "id": "AZRzHbwUnjgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.4 분석 대상이 되는 단어를 리스트에 요인으로 넣을 때 정규표현식을 사용하여 더 간단하고, 범위가 넓게 검색할 수 있다.  \n",
        "- 분석 대상 단어 표현에 정규표현식을 포함함과 동시에 Concordance ( ) 함수에 마지막 인자에 regex = True 를 추가한다. Collocation (단어 동반 출현) 검색 인자를 추가한다 (예시: [\"run.*\", \"ran\"]... regex = True) \n",
        "- Regular Expression 기호 해설\n",
        "-- 임의의 문자 (.) \n",
        "-- 문자수가 0~무한대 (*)\n",
        "--Q: 임의의 문자 (.)을 넣지 않고 (*)만 추가했을 때에는 분석 대상 단어에 세번째 철자가 n 이 아닌 \"rulers, ruffles, russia, ruger\" 등등의 단어도 검색되어 결과로 나옴. 왜요??? "
      ],
      "metadata": {
        "id": "tWn3vaXEQ_YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results3 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run.*\",\"ran\"],collocates = [\"quick.*\"], nhits = 10, regex = True)\n",
        "for x in conc_results3:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "id": "Bf2gOwCnoDCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.5 검색 결과를 새파일로 저장하기 (예시: \"run_25.txt\")\n",
        "\n",
        "- concordance 코드로 출력한 결과를 새파일로 저장할 때에는, concord( ) 함수 마지막에 새로 저장할 파일 이름을 outcome 변수를 사용하여 지정해 준다 (예시: outname = \"run_25.txt\").\n",
        "\n",
        "- 전체 결과를 저장하기 위해서는 \"nhits = 숫자\" 인자는 포함 안한다."
      ],
      "metadata": {
        "id": "JWs-XSoKpr1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write concordance lines to a file called \"run_25.txt\"\n",
        "conc_results4 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"], outname = \"run_25.txt\")"
      ],
      "metadata": {
        "id": "MCU4o-D4oQo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Corpus_Toolkit 사용하여 Tagging (단어에 품사를 연결)\n",
        "\n",
        "###6.1 단어에 품사를 연결한 결과를 새파일로 저장하기 \n",
        "\n",
        "#####[코드해석 1] tag ( ) 함수\n",
        "파일을 읽어 불러들여들여 태그하여 tagged_brown 변수에 할당한다.\n",
        "\n",
        "#####[코드해석 2]  write_corpus (A, B) 함수\n",
        "- A 인자: \" \" 안에 새파일이름을 넣기 \n",
        "- B 인자: 파일 내용을 채울 변수를 넣는다.\n",
        "\n",
        "#####[코드해석 3] Nest 기법으로 위의 파일 불러들여 읽고 태그하기 [코드 1]과 태그한 결과를 새파일로 만들기 [코드 2]를 한 줄에 작성"
      ],
      "metadata": {
        "id": "NN3_IsC5W6ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_brown = ct.tag(ct.ldcorpus(\"brown_single\"))\n",
        "ct.write_corpus(\"tagged_brown_single\",tagged_brown) #the first argument is the folder where the tagged files will be written"
      ],
      "metadata": {
        "id": "q_HExln_qlMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ct.write_corpus(\"tagged_brown_single\",ct.tag(ct.ldcorpus(\"brown_single\")))"
      ],
      "metadata": {
        "id": "ubYWE2zDsvYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6.2 텍스트에 나오는 단어에 픔사를 연결한 것에서 품사의 빈도수를 조사하기\n",
        "\n",
        "#####[코드해석 1] frequency ( ) 함수\n",
        "태깅한 단어-품사를 저장한 파일을 다시 불러들이고 품사 빈도수를 계산하여 이를 tagged_freq 변수에 할당한다.  \n",
        "\n",
        "#####[코드해석 2] head (A, B) 함수\n",
        "A 인자: 빈도수가 계산된 tagged_freq 변수\n",
        "B 인자: hits = 숫자 지정 "
      ],
      "metadata": {
        "id": "rQSMc4jTe_gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_freq = ct.frequency(ct.reload(\"tagged_brown_single\"))\n",
        "ct.head(tagged_freq, hits = 10)"
      ],
      "metadata": {
        "id": "syhi8MT0s7xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Collocate (단어 동반 출현) \n",
        "####[Collocate 이해에 도움되는 문서](\"https://hecc.ubc.ca/quantitative-textual-analysis/qta-practice/what-antconcs-concordance-tool-can-do-for-you/\")\n",
        "\n",
        "특정 단어와 단어 동반 출현하는 단어의 빈도 지수를 분석 후 상위 10 개 결과 출력\n",
        "\n",
        "#####[코드해석 1] ct 모듈 collocator (A, B, C) 함수를 사용\n",
        "- A 인자: 토큰화된 단어 (예시: tokenize ( ) 함수)\n",
        "- B 인자: \"대상 단어\" (예시: \"go\")\n",
        "- C 인자: Sort by (정렬방식). 기본형은 stat. stat 의 기본형은 MI (mutual information (상호정보)) (예시: stat = \"MI\")\n",
        "-- C 인자 추가설명: The default “Sort By” is “Stat”. And the default test statistic is MI, or Mutual Information. Mutual Information represents a ratio of the observed frequency (fo) of the combination of two words (or two word phrases) divided by the expected frequency (fe) of the combination: fo / fe .\n",
        "\n",
        "#####[코드해석 2] ct 모듈 head (A, B) 함수 사용\n",
        "- A 인자: 동반단어결과 변수\n",
        "- B 인자: hits = 숫자\n",
        "\n"
      ],
      "metadata": {
        "id": "BtrKQoRHskKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collocates = ct.collocator(ct.tokenize(ct.ldcorpus(\"brown_single\")),\"go\",stat = \"MI\")\n",
        "#stat options include: \"MI\", \"T\", \"freq\", \"left\", and \"right\"\n",
        "\n",
        "ct.head(collocates, hits = 10)"
      ],
      "metadata": {
        "id": "TWCovRiTtJtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#First, generate frequency lists for each corpus\n",
        "corp1freq = ct.frequency(ct.tokenize(ct.ldcorpus(\"corp1\")))\n",
        "corp2freq = ct.frequency(ct.tokenize(ct.ldcorpus(\"corp2\")))\n",
        "\n",
        "#then calculate Keyness\n",
        "corp_key = ct.keyness(corp1freq,corp2freq, effect = \"log-ratio\")\n",
        "ct.head(corp_key, hits = 10) #to display top hits\n"
      ],
      "metadata": {
        "id": "HcUhyymttUi9",
        "outputId": "4a572caa-f4b1-47a0-84d0-50233cf37266",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No files found. There may be a problem with your working directory or your file search term.\n",
            "No files found. There may be a problem with your working directory or your file search term.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. N-grams (N-그램)\n",
        "N-그램은 몇 개 단어가 이웃 하여 연속되어 나오는 것이다. tokenize ( ) 함수에서 ngram 인자를 포함하여 분석해서 이웃한 단어의 연속을 토큰화 한다. 각각의 토큰화된 n-그램에 대한 빈도수를 측정 후 상위 10 개 결과 출력 \n",
        "#####[코드해석 1] ct 모듈 tokenize (A, B, C) 함수를 사용 \n",
        "- A 인자: 분석할 대상 텍스트 불러들여 읽기\n",
        "- B 인자: 표준 단어 사용 여부 (예시: lemma = False)\n",
        "- C 인자: 이웃 단어 숫자 지정 (예시: ngram = 숫자): 숫자에 3을 넣으면 3 개의 이웃한 단어를 분석함  \n",
        "  \n",
        "#####[코드해석 2] ct 모듈 frequency ( ) 함수를 사용하여 토큰화된 각각의 n-그램에 대한 빈도수를 계산한다. \n",
        "\n",
        "#####[코드해석 3] ct 모듈 head (A, B) 함수 사용\n",
        "- A 인자: 토큰화된 각각의 n-gram 빈도수 결과 변수\n",
        "- B 인자: hits = 숫자 (예시: 10 개 결과 출력)\n",
        "\n",
        "[코드 1]과 [코드 2]를 Nest 기법으로 한 줄 명령도 가능\n"
      ],
      "metadata": {
        "id": "AC3sbfm_uRO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigramfreq = ct.frequency(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False, ngram = 3))\n",
        "ct.head(trigramfreq, hits = 10)"
      ],
      "metadata": {
        "id": "Ll9zKBO4uAo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9. Dependency bigrams\n",
        "주요어-의존어 이웃하여 연속으로 오는 것이다. 통사적으로 Head (주요어)와 Dependency (의존어)는 연결되어 있다. \"The player kicked the ball.\" 문장에서 \"kicked\" 와 \"ball\"은 통사적으로 동사와 직접 목적어 관계에 있고, 동사는 주요어, 목적어는 의존어라고 본다. dep_bigram ( ) 함수는 문장을 범위로 하여 주요어-의존어를 찾는다. \n",
        "\n",
        "dep_bigram ( ) 함수를 사용하여 주요어-의존어 관계를 찾고, 이 단어 쌍들의 빈도수를 계산하며 상위 10 개 결과 출력\n",
        "\n",
        "#####[코드해석 1] ct 모듈 dep_bigram (A,B) 함수 사용\n",
        "- A 인자: 분석할 대상 텍스트를 불러들여 읽기\n",
        "- B 인자: 의존어 지정. direct object (직접목적어)를 인자로 사용할 때는 \" \" 안에 dobj 를 넣는다. (예시: \"dobj\") \n",
        "\n",
        "#####[코드해석 2] ct 모듈 head (A, B) 함수 사용\n",
        "- A 인자: 주요어-의존어 변수에 [\"bi_freq\"]를 써서 해당 변수의 빈도수 계산을 가능하게 한다.\n",
        "- B 인자: hits = 숫자 (예시: 10 개 결과 출력)\n",
        "\n"
      ],
      "metadata": {
        "id": "js4OUwWuubZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bg_dict = ct.dep_bigram(ct.ldcorpus(\"brown_single\"),\"dobj\")\n",
        "ct.head(bg_dict[\"bi_freq\"], hits = 10)\n",
        "#other keys include \"dep_freq\", \"head_freq\", and \"range\"\n",
        "#also note that the key \"samples\" can be used to obtain a list of sample sentences\n",
        "#but, this is not compatible with the ct.head() function (see ct.dep_conc() instead)"
      ],
      "metadata": {
        "id": "BJy-ilc-uf4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9. Strength of association (연결강도)\n",
        "주요어-의존어에 대하여 두 단어의 연결강도를 측정할 수 있다. \n",
        "\n",
        "soa( ) 함수는 dep_bigram ( ) 함수 결과 (즉, 주요어-의존어를 쌍으로 출력)를 사용하여 각각의 두 단어 쌍간에 연결강도를 계산한다.  \n",
        "\n",
        "#####[코드해석 1] ct 모듈 soa (A, B) 함수\n",
        "- A 인자: dep_bigram ( ) 함수 결과 변수\n",
        "- B 인자: Sort by (정렬방식). 기본형은 stat. stat 의 기본형은 MI (mutual information (상호정보)) (예시: stat = \"MI\")\n",
        "\n",
        "#####[코드해석 2] ct 모듈 head (A, B) 함수 사용\n",
        "- A 인자: 사전형태로 출력된 주요어-의존어 사이의 연결강도 변수\n",
        "- B 인자: hits = 숫자 (예시: 10 개 결과 출력)\n"
      ],
      "metadata": {
        "id": "UewcMQUOuxRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soa_mi = ct.soa(bg_dict,stat = \"MI\")\n",
        "#other stat options include: \"T\", \"faith_dep\", \"faith_head\",\"dp_dep\", and \"dp_head\"\n",
        "ct.head(soa_mi, hits = 10)"
      ],
      "metadata": {
        "id": "CNvuN00Cu1Da",
        "outputId": "d62bac39-40c6-4a14-d98f-7baae02097aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class_judge\t10.920684367813834\n",
            "nose_scratch\t10.715966457570863\n",
            "suicide_commit\t10.378096818814479\n",
            "nose_blow\t10.171645941347052\n",
            "imagination_capture\t9.963753089705719\n",
            "calendar_adjust\t9.908611535513257\n",
            "English_speak\t9.323649034792101\n",
            "resemblance_bear\t9.253259706900703\n",
            "throat_clear\t9.1501662139366\n",
            "expense_deduct\t9.1501662139366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10. dep_conc ( ) 함수는 dep_bigram ( ) 함수 결과 (즉, 주요어-의존어를 쌍으로 출력)를 사용하여 각각의 두 단어 쌍의 용례 내용이 들어 있는 새파일을 현재 작업 디렉토리에 생성한다. \n",
        "\n",
        "#####[코드해석 1] ct 모듈 dep_conc (A, B) 함수\n",
        "- A 인자: dep_bigram ( ) 함수 결과 변수 옆에 [\"samples\"]를 쓰면 기본값으로 50 개 샘플이 무작위로 생성\n",
        "- B 인자: \" \" 안에 dobj_results.html 넣어 새파일명과 포맷을 지정 \n",
        "\n",
        "단어 용법을 분석할 수 있는 AntConc 는 GUI (그래픽 사용자 인터페이스)를 기반으로 하고 사용자 선호도가 높고 무료로 이용 가능하다.\n",
        "[AntConc 사용 문학 분석 예시](\"https://hecc.ubc.ca/quantitative-textual-analysis/qta-practice/what-antconcs-concordance-tool-can-do-for-you/\")\n",
        "\n",
        "Q: html 파일을 웹상에서 어떻게 열어 보나요? "
      ],
      "metadata": {
        "id": "G9iry7cpvEbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ct.dep_conc(bg_dict[\"samples\"],\"dobj_results\")"
      ],
      "metadata": {
        "id": "Nxh0aFGou9cP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}